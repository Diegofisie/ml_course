{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML Course, Bogotá, Colombia  (&copy; Josh Bloom; June 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       ".rendered_html\n",
       "{\n",
       "  color: #2C5494;\n",
       "  font-family: Ubuntu;\n",
       "  font-size: 140%;\n",
       "  line-height: 1.1;\n",
       "  margin: 0.5em 0;\n",
       "  }\n",
       "\n",
       ".talk_title\n",
       "{\n",
       "  color: #498AF3;\n",
       "  font-size: 250%;\n",
       "  font-weight:bold;\n",
       "  line-height: 1.2; \n",
       "  margin: 10px 50px 10px;\n",
       "  }\n",
       "\n",
       ".subtitle\n",
       "{\n",
       "  color: #386BBC;\n",
       "  font-size: 180%;\n",
       "  font-weight:bold;\n",
       "  line-height: 1.2; \n",
       "  margin: 20px 50px 20px;\n",
       "  }\n",
       "\n",
       ".slide-header, p.slide-header\n",
       "{\n",
       "  color: #498AF3;\n",
       "  font-size: 200%;\n",
       "  font-weight:bold;\n",
       "  margin: 0px 20px 10px;\n",
       "  page-break-before: always;\n",
       "  text-align: center;\n",
       "  }\n",
       "\n",
       ".rendered_html h1\n",
       "{\n",
       "  color: #498AF3;\n",
       "  line-height: 1.2; \n",
       "  margin: 0.15em 0em 0.5em;\n",
       "  page-break-before: always;\n",
       "  text-align: center;\n",
       "  }\n",
       "\n",
       "\n",
       ".rendered_html h2\n",
       "{ \n",
       "  color: #386BBC;\n",
       "  line-height: 1.2;\n",
       "  margin: 1.1em 0em 0.5em;\n",
       "  }\n",
       "\n",
       ".rendered_html h3\n",
       "{ \n",
       "  font-size: 100%;\n",
       "  line-height: 1.2;\n",
       "  margin: 1.1em 0em 0.5em;\n",
       "  }\n",
       "\n",
       ".rendered_html li\n",
       "{\n",
       "  line-height: 1.8;\n",
       "  }\n",
       "\n",
       ".input_prompt, .CodeMirror-lines, .output_area\n",
       "{\n",
       "  font-family: Consolas;\n",
       "  font-size: 120%;\n",
       "  }\n",
       "\n",
       ".gap-above\n",
       "{\n",
       "  padding-top: 200px;\n",
       "  }\n",
       "\n",
       ".gap01\n",
       "{\n",
       "  padding-top: 10px;\n",
       "  }\n",
       "\n",
       ".gap05\n",
       "{\n",
       "  padding-top: 50px;\n",
       "  }\n",
       "\n",
       ".gap1\n",
       "{\n",
       "  padding-top: 100px;\n",
       "  }\n",
       "\n",
       ".gap2\n",
       "{\n",
       "  padding-top: 200px;\n",
       "  }\n",
       "\n",
       ".gap3\n",
       "{\n",
       "  padding-top: 300px;\n",
       "  }\n",
       "\n",
       ".emph\n",
       "{\n",
       "  color: #386BBC;\n",
       "  }\n",
       "\n",
       ".warn\n",
       "{\n",
       "  color: red;\n",
       "  }\n",
       "\n",
       ".center\n",
       "{\n",
       "  text-align: center;\n",
       "  }\n",
       "\n",
       ".nb_link\n",
       "{\n",
       "    padding-bottom: 0.5em;\n",
       "}\n",
       "\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ../talktools.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly/Novelty/Outlier Detection\n",
    "\n",
    "Sometimes we want to find outliers in our data -- either as candidates for cleaning or because novelties are what we are most interested in.  What would you do to find novelties? One possibility is to fit the data (potentially with \"leave one out\") you have with some parametric function and inspect those data that are farthest from your fit. You could assign a novelty score based on distance to the fit values.\n",
    "\n",
    "\n",
    "We can also look to some ML techniques to create a non-parametric model of anomalies, potentially  There's not a lot of machinery for this in `sklearn` but there is some.\n",
    "\n",
    "We'll look at *Isolation Forests* here as one technique. This requires, of course, that you've got a featurized dataset.\n",
    "\n",
    "See http://scikit-learn.org/stable/auto_examples/covariance/plot_outlier_detection.html#sphx-glr-auto-examples-covariance-plot-outlier-detection-py\n",
    "\n",
    "see also C. C. Aggarwal and S. Sathe, “Theoretical foundations and algorithms for outlier ensembles.” ACM SIGKDD Explorations Newsletter, vol. 17, no. 1, pp. 24–47, 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://image.slidesharecdn.com/gerster-papisconnect-anomaly-150521055051-lva1-app6891/95/anomaly-detection-with-bigml-4-638.jpg?cb=1432187570\">\n",
    "<img src=\"https://www.evernote.com/l/AUUwXmpu3nVNOpxwEo67YZD8VZwI950BtuMB/image.png\">\n",
    "Source: http://www.slideshare.net/DavidGerster1/anomaly-detection-with-bigml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.datasets import load_digits\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = IsolationForest(bootstrap=True, random_state=42, max_features=1.0)\n",
    "clf.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = clf.decision_function(X)\n",
    "most_wierd = np.argsort(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,5,figsize=(10,6))\n",
    "for i,ind in enumerate(most_wierd[:5]):\n",
    "    axs[i].imshow(X[ind].reshape(8,8),interpolation=\"nearest\",cmap=plt.cm.gray_r)\n",
    "    axs[i].set_title(\"{0:n} score={1:0.3f}\".format(y[ind],scores[ind]))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,5,figsize=(10,6))\n",
    "for i,ind in enumerate(most_wierd[-5:]):\n",
    "    axs[i].imshow(X[ind].reshape(8,8),interpolation=\"nearest\",cmap=plt.cm.gray_r)\n",
    "    axs[i].set_title(\"{0:n} score={1:0.3f}\".format(y[ind],scores[ind]))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('http://www.bigmacc.info', width=\"100%\", height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://scikit-learn.org/stable/auto_examples/plot_anomaly_comparison.html#sphx-glr-auto-examples-plot-anomaly-comparison-py\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_anomaly_comparison_001.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier for Time series\n",
    "\n",
    "<img src=\"https://github.com/twitter/AnomalyDetection/raw/master/figs/Fig1.png\">\n",
    "\n",
    "There are functional-form modelling approaches to this.\n",
    "\n",
    "For example, from Twitter's AnomalyDetection algorithm (in R): \"The underlying algorithm – referred to as Seasonal Hybrid ESD (S-H-ESD) builds upon the Generalized [Extreme Studentized deviate (ESD)](https://www.itl.nist.gov/div898/handbook/eda/section3/eda35h3.htm) test for detecting anomalies. Note that S-H-ESD can be used to detect both global as well as local anomalies. This is achieved by employing time series decomposition and using robust statistical metrics, viz., median together with ESD. \"\n",
    "\n",
    "https://blog.twitter.com/engineering/en_us/a/2015/introducing-practical-and-robust-anomaly-detection-in-a-time-series.html\n",
    "\n",
    "People have implemented this in Python, thankfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/Marcnuth/AnomalyDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def dparserfunc(date):\n",
    "        return pd.datetime.strptime(date, '%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "df = pd.read_csv(\"https://github.com/Marcnuth/AnomalyDetection\"\n",
    "                            \"/raw/master/resources/data/test_detect_anoms.csv\",\n",
    "                            index_col='timestamp',\n",
    "                            parse_dates=True, squeeze=True,\n",
    "                            date_parser=dparserfunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anomaly_detection\n",
    "import numpy as np\n",
    "result = anomaly_detection.anomaly_detect_ts(df, max_anoms=0.02, direction=\"both\", plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.plot(figsize=(12,5),alpha=0.5)\n",
    "t = result[\"anoms\"].index\n",
    "v = result[\"anoms\"].values\n",
    "ax.scatter(t,v,c=\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holt Winters & Exponential Smoothing\n",
    "\n",
    "> Whereas in the simple moving average the past observations are weighted equally, exponential functions are used to assign exponentially decreasing weights over time. It is an easily learned and easily applied procedure for making some determination based on prior assumptions by the user, such as seasonality. Exponential smoothing is often used for analysis of time-series data. - [Wikipedia](https://en.wikipedia.org/wiki/Exponential_smoothing)\n",
    "\n",
    "<img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/01/eq.png\">\n",
    "\n",
    "From: https://www.analyticsvidhya.com/blog/2018/02/time-series-forecasting-methods/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda update statsmodels -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels\n",
    "statsmodels.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing, Holt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit1 = ExponentialSmoothing(np.asarray(df.values), seasonal_periods=10, trend='add', seasonal='add',).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit1.fittedvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.plot(figsize=(12,5),alpha=0.5)\n",
    "#ax.plot(df.index, fit1.fittedvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAD(x):\n",
    "    \"\"\" median absolute deviation \"\"\"\n",
    "    return np.median(np.abs(x))\n",
    "    \n",
    "md = MAD(df.values - fit1.fittedvalues)\n",
    "deviation = 8\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(df.index, df.values - fit1.fittedvalues)\n",
    "plt.ylabel(\"residuals\")\n",
    "plt.axhline(md*deviation, c=\"r\")\n",
    "plt.axhline(-md*deviation, c=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = np.argwhere((df.values - fit1.fittedvalues > deviation*md) | (df.values - fit1.fittedvalues < -deviation*md))\n",
    "print(len(outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.plot(figsize=(12,5),alpha=0.5)\n",
    "ax.scatter(df.iloc[outliers[:,0]].index, df.iloc[outliers[:,0]].values, c=\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-encoders for Anomaly Detection\n",
    "\n",
    "The ability for an auto-encode to compress and then decompress without loss of fidelity is a non-parametric way of making predictions. If our predictions are bad (ie. the loss is large) then we might call this an anomaly.  The following derives from the blogpost: https://towardsdatascience.com/machine-learning-for-anomaly-detection-and-condition-monitoring-d4614e7de770\n",
    "\n",
    "Here we have vibration data recorded for a machine. The machine is run until it breaks. The task is to find out when the system goes out of normal operations, which will eventually lead to a break/failure.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*7QfUsut5-OeSymRK_Z8mNg.jpeg\">\n",
    "<center><i>Normal operations over the first few days</i></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below to make the timeseries set merged_dataset_BearingTest_2.csv\n",
    "\"\"\"\n",
    "# To prep the data, aownload at http://data-acoustics.com/measurements/bearing-faults/bearing-4/\n",
    "# then: 7za x IMS.7z \n",
    "# then:  unrar x 2nd_test.rar\n",
    "data_dir = '/Users/jbloom/Downloads/2nd_test'\n",
    "merged_data = pd.DataFrame()\n",
    "\n",
    "for filename in os.listdir(data_dir):\n",
    "    print(filename)\n",
    "    dataset=pd.read_csv(os.path.join(data_dir, filename), sep='\\t')\n",
    "    dataset_mean_abs = np.array(dataset.abs().mean())\n",
    "    dataset_mean_abs = pd.DataFrame(dataset_mean_abs.reshape(1,4))\n",
    "    dataset_mean_abs.index = [filename]\n",
    "    merged_data = merged_data.append(dataset_mean_abs)\n",
    "\n",
    "merged_data.columns = ['Bearing 1','Bearing 2','Bearing 3','Bearing 4']\n",
    "\n",
    "merged_data.index = pd.to_datetime(merged_data.index, format='%Y.%m.%d.%H.%M.%S')\n",
    "merged_data = merged_data.sort_index()\n",
    "merged_data.to_csv('merged_dataset_BearingTest_2.csv')\n",
    "\"\"\"\n",
    "merged_data = pd.read_csv(\"merged_dataset_BearingTest_2.csv\", index_col=0, parse_dates=True)\n",
    "print(merged_data.head())\n",
    "print(merged_data.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the data spans about a week with vibrations measurements recorded every 10 minutes.  Let's make training data be the normal behavior and the testing data include the normal behavior as it transitions into abnormal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = merged_data['2004-02-12 11:02:39':'2004-02-13 23:52:39']\n",
    "dataset_test = merged_data['2004-02-13 23:52:39':]\n",
    "\n",
    "dataset_test.plot(figsize = (12,6))\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"vibration level\")\n",
    "plt.title(\"The days leading up to the failure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's scale this using the `sklearn.MinMaxScaler`. See https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_train = pd.DataFrame(scaler.fit_transform(dataset_train), \n",
    "                              columns=dataset_train.columns, \n",
    "                              index=dataset_train.index)\n",
    "# Random shuffle training data\n",
    "X_train.sample(frac=1)\n",
    "\n",
    "X_test = pd.DataFrame(scaler.transform(dataset_test), \n",
    "                             columns=dataset_test.columns, \n",
    "                             index=dataset_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build an extremely simple autocoder -- four bearing measure inputs (ie. every timestamp). (We could do a more involved encoder where we chunk up time snippets.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(10)\n",
    "act_func = 'relu'\n",
    "\n",
    "# Input layer:\n",
    "model=Sequential()\n",
    "\n",
    "# First hidden layer, connected to input vector X. \n",
    "model.add(Dense(10,activation=act_func,\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                kernel_regularizer=regularizers.l2(0.05),\n",
    "                input_shape=(X_train.shape[1],)\n",
    "               )\n",
    "         )\n",
    "\n",
    "model.add(Dense(2,activation=act_func,\n",
    "                kernel_initializer='glorot_uniform'))\n",
    "\n",
    "model.add(Dense(10,activation=act_func,\n",
    "                kernel_initializer='glorot_uniform'))\n",
    "\n",
    "model.add(Dense(X_train.shape[1],\n",
    "                kernel_initializer='glorot_uniform'))\n",
    "\n",
    "model.compile(loss='mse',optimizer='adam')\n",
    "\n",
    "# Train model for 100 epochs, batch size of 10: \n",
    "NUM_EPOCHS=100\n",
    "BATCH_SIZE=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "earlystop = tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, \n",
    "                                             patience=10, \\\n",
    "                                             verbose=1, mode='auto')\n",
    "\n",
    "history=model.fit(np.array(X_train),np.array(X_train),\n",
    "                  batch_size=BATCH_SIZE, \n",
    "                  epochs=NUM_EPOCHS,\n",
    "                  validation_split=0.05,\n",
    "                  verbose = 1, callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'],\n",
    "         'b',\n",
    "         label='Training loss')\n",
    "plt.plot(history.history['val_loss'],\n",
    "         'r',\n",
    "         label='Validation loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss, [mse]')\n",
    "plt.ylim([0,.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = model.predict(np.array(X_train))\n",
    "X_pred = pd.DataFrame(X_pred, \n",
    "                      columns=X_train.columns)\n",
    "X_pred.index = X_train.index\n",
    "\n",
    "scored = pd.DataFrame(index=X_train.index)\n",
    "scored['Loss_mae'] = np.mean(np.abs(X_pred-X_train), axis = 1)\n",
    "plt.figure()\n",
    "sns.distplot(scored['Loss_mae'],\n",
    "             bins = 10, \n",
    "             kde= True,\n",
    "            color = 'blue');\n",
    "plt.xlim([0.0,.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like normal behavior has a loss below 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = model.predict(np.array(X_test))\n",
    "X_pred = pd.DataFrame(X_pred, \n",
    "                      columns=X_test.columns)\n",
    "X_pred.index = X_test.index\n",
    "\n",
    "scored = pd.DataFrame(index=X_test.index)\n",
    "scored['Loss_mae'] = np.mean(np.abs(X_pred-X_test), axis = 1)\n",
    "scored['Threshold'] = 0.3\n",
    "scored['Anomaly'] = scored['Loss_mae'] > scored['Threshold']\n",
    "scored.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred_train = model.predict(np.array(X_train))\n",
    "X_pred_train = pd.DataFrame(X_pred_train,  columns=X_train.columns)\n",
    "X_pred_train.index = X_train.index\n",
    "\n",
    "scored_train = pd.DataFrame(index=X_train.index)\n",
    "scored_train['Loss_mae'] = np.mean(np.abs(X_pred_train-X_train), axis = 1)\n",
    "scored_train['Threshold'] = 0.3\n",
    "scored_train['Anomaly'] = scored_train['Loss_mae'] > scored_train['Threshold']\n",
    "scored = pd.concat([scored_train, scored])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored.plot(logy=True,  figsize = (10,6), ylim = [1e-2,1e2], color = ['blue','red'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
