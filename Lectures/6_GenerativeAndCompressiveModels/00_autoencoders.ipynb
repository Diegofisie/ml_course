{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML Course, Bogot√°, Colombia  (&copy; Josh Bloom; June 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       ".rendered_html\n",
       "{\n",
       "  color: #2C5494;\n",
       "  font-family: Ubuntu;\n",
       "  font-size: 140%;\n",
       "  line-height: 1.1;\n",
       "  margin: 0.5em 0;\n",
       "  }\n",
       "\n",
       ".talk_title\n",
       "{\n",
       "  color: #498AF3;\n",
       "  font-size: 250%;\n",
       "  font-weight:bold;\n",
       "  line-height: 1.2; \n",
       "  margin: 10px 50px 10px;\n",
       "  }\n",
       "\n",
       ".subtitle\n",
       "{\n",
       "  color: #386BBC;\n",
       "  font-size: 180%;\n",
       "  font-weight:bold;\n",
       "  line-height: 1.2; \n",
       "  margin: 20px 50px 20px;\n",
       "  }\n",
       "\n",
       ".slide-header, p.slide-header\n",
       "{\n",
       "  color: #498AF3;\n",
       "  font-size: 200%;\n",
       "  font-weight:bold;\n",
       "  margin: 0px 20px 10px;\n",
       "  page-break-before: always;\n",
       "  text-align: center;\n",
       "  }\n",
       "\n",
       ".rendered_html h1\n",
       "{\n",
       "  color: #498AF3;\n",
       "  line-height: 1.2; \n",
       "  margin: 0.15em 0em 0.5em;\n",
       "  page-break-before: always;\n",
       "  text-align: center;\n",
       "  }\n",
       "\n",
       "\n",
       ".rendered_html h2\n",
       "{ \n",
       "  color: #386BBC;\n",
       "  line-height: 1.2;\n",
       "  margin: 1.1em 0em 0.5em;\n",
       "  }\n",
       "\n",
       ".rendered_html h3\n",
       "{ \n",
       "  font-size: 100%;\n",
       "  line-height: 1.2;\n",
       "  margin: 1.1em 0em 0.5em;\n",
       "  }\n",
       "\n",
       ".rendered_html li\n",
       "{\n",
       "  line-height: 1.8;\n",
       "  }\n",
       "\n",
       ".input_prompt, .CodeMirror-lines, .output_area\n",
       "{\n",
       "  font-family: Consolas;\n",
       "  font-size: 120%;\n",
       "  }\n",
       "\n",
       ".gap-above\n",
       "{\n",
       "  padding-top: 200px;\n",
       "  }\n",
       "\n",
       ".gap01\n",
       "{\n",
       "  padding-top: 10px;\n",
       "  }\n",
       "\n",
       ".gap05\n",
       "{\n",
       "  padding-top: 50px;\n",
       "  }\n",
       "\n",
       ".gap1\n",
       "{\n",
       "  padding-top: 100px;\n",
       "  }\n",
       "\n",
       ".gap2\n",
       "{\n",
       "  padding-top: 200px;\n",
       "  }\n",
       "\n",
       ".gap3\n",
       "{\n",
       "  padding-top: 300px;\n",
       "  }\n",
       "\n",
       ".emph\n",
       "{\n",
       "  color: #386BBC;\n",
       "  }\n",
       "\n",
       ".warn\n",
       "{\n",
       "  color: red;\n",
       "  }\n",
       "\n",
       ".center\n",
       "{\n",
       "  text-align: center;\n",
       "  }\n",
       "\n",
       ".nb_link\n",
       "{\n",
       "    padding-bottom: 0.5em;\n",
       "}\n",
       "\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ../talktools.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders\n",
    "\n",
    "A form of non-parametric representation learning with neutral nets, where the architecture of the network is used to reduce the dimensionality of the data. We will see forms of parametric dimensionality reduction in Lecture 7. \n",
    "\n",
    "As the name suggests, autoencoders uses the data itself to learn the best way to represent it in a compact way--it's a form of semantic compression. This is a family of self- (or un-) supervised modeling.\n",
    "\n",
    "<img src=\"https://lilianweng.github.io/lil-log/assets/images/autoencoder-architecture.png\">\n",
    "Source: https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html\n",
    "\n",
    "In practice, we take an input X (which may be a 1-d vector, 2-d image, ...) and try to squeeze it down to a smaller number of values in the \"bottleneck\" layer and then uncompress back to it's original shape and form. The loss function that we construct will be the way in which the network learns on each backprop through the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a autoencoder which uses convnets to restruct the fashion dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Reshape, \\\n",
    "                                                            Activation, BatchNormalization, UpSampling2D\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# Print keras version\n",
    "print(tensorflow.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "nb_classes = 10\n",
    "batch_size = 128\n",
    "bottleneck_size = 64\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # scale the images to 0-1\n",
    "\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train =  to_categorical(y_train, nb_classes)\n",
    "Y_test =  to_categorical(y_test, nb_classes)\n",
    "\n",
    "input_shape = x_train[0].shape  + (1,)\n",
    "input_shape\n",
    "input_img = Input(shape = (28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
    "\n",
    "x = Flatten()(x)\n",
    "bottleneck = Dense(bottleneck_size, name=\"bottleneck\")(x)\n",
    "\n",
    "x = Dense(128)(bottleneck)\n",
    "x = Reshape((4,4,8))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "# add with tf.device('/gpu:0'): if on GPU\n",
    "autoencoder = Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow \n",
    "\n",
    "logdir = os.path.join(\"nn_results\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "run_time_string = datetime.datetime.utcnow().isoformat(timespec='minutes')\n",
    "\n",
    "model_path = f'nn_results/colombia_autoencoder_nn_{run_time_string}.h5'\n",
    "print(f\"Training ... {model_path}\")\n",
    "\n",
    "\n",
    "model_check = tf.keras.callbacks.ModelCheckpoint(model_path,\n",
    "        monitor='val_loss', \n",
    "        save_best_only=True, \n",
    "        mode='min',\n",
    "        verbose=1)\n",
    "\n",
    "tensorboard_callback = tensorflow.keras.callbacks.TensorBoard(logdir, \n",
    "                                                              histogram_freq=0, \n",
    "                                                              write_graph=True, \n",
    "                                                              write_grads=False, \n",
    "                                                              write_images=False, \n",
    "                 embeddings_freq=0, \n",
    "                 embeddings_layer_names=None, \n",
    "                 embeddings_metadata=None, embeddings_data=None)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "autoencoder_train = autoencoder.fit(x_train, x_train, \n",
    "                                                          batch_size=batch_size,epochs=10,\n",
    "                                                          verbose=1, shuffle=False, \n",
    "                                                          validation_data=(x_test, x_test),\n",
    "                                                          callbacks=[tensorboard_callback, model_check])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs = autoencoder.predict(x_test)\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i+1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + n + 1)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(input_img, bottleneck)\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 8))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(1, n, i+1)\n",
    "    plt.imshow(encoded_imgs[i].reshape(8, 8).T, cmap=\"viridis\")\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of the above as the (encoded) high-level concept of each image. Can you see the similarities across classes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we do with this network?\n",
    "\n",
    "  - **Remote compression** - if we have the encoder on our phone (or some other device) we can encode images there and send the bottleneck elsewhere (more than a factor of 10 compression with this network). We can later decode the data elsewhere to get back a fair representation of the original data. This is a great bandwidth saver! It also creates an interesting level of anonymity (if the bottleneck data is intercepted, no one without the decoder could figure out what the original images looked like).\n",
    "   \n",
    "  - **Classification** - we just built 64 features without using, for example, our knowledge of computer vision. And we avoiding coding 64 separate features.\n",
    "  \n",
    "  - **Clustering** - this is a non-parametric way of finding low-dimensional embedding of our data. We'll see more of this in Lecture 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakout\n",
    "\n",
    "a) Using the autoencoder model above, create a random forest model to predict the classes of the images using the 64-parameter bottleneck layer. What accuracy do you get? How does it compare with the accuracy we got on the `convnet` model before?\n",
    "\n",
    "b) Experiment with trying a different sized layer (e.g., size 4, 16, 32) and repeat step a) above. Do you see any trends with bottleneck size?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising Autoencoder\n",
    "\n",
    "Another application of autoencoders is to construct a model which takes low signal-to-noise data and produces high signal-to-noise versions.  For example, https://thenextweb.com/insider/2017/02/08/google-figured-out-a-way-to-zoom-and-enhance-photos-just-like-in-the-movies/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('I_8ZH1Ggjk0?t=27')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the model is very similar as before except that we feed in a corrupted/low SNR version in the input and compare the output against a higher fidelity version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_factor = 0.3\n",
    "x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) \n",
    "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) \n",
    "\n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i+1)\n",
    "    plt.imshow(x_train_noisy[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display high SNR version\n",
    "    ax = plt.subplot(2, n, i + n + 1)\n",
    "    plt.imshow(x_train[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_time_string = datetime.datetime.utcnow().isoformat(timespec='minutes')\n",
    "\n",
    "model_path = f'nn_results/colombia_denoise_autoencoder_nn_{run_time_string}.h5'\n",
    "print(f\"Training ... {model_path}\")\n",
    "\n",
    "\n",
    "model_check = tf.keras.callbacks.ModelCheckpoint(model_path,\n",
    "        monitor='val_loss', \n",
    "        save_best_only=True, \n",
    "        mode='min',\n",
    "        verbose=1)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "autoencoder_train = autoencoder.fit(x_train_noisy, x_train, \n",
    "                                                          batch_size=batch_size,epochs=25,\n",
    "                                                          verbose=1, shuffle=True, \n",
    "                                                          validation_data=(x_test_noisy, x_test),\n",
    "                                                          callbacks=[tensorboard_callback, model_check])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs = autoencoder.predict(x_test_noisy)\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i+1)\n",
    "    plt.imshow(x_test_noisy[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + n + 1)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder\n",
    "\n",
    "> A VAE \"is an autoencoder that learns a latent variable model for its input data. So instead of letting your neural network learn an arbitrary function, you are learning the parameters of a probability distribution modeling your data. If you sample points from this distribution, you can generate new input data samples: a VAE is a \"generative model\". -- https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "\n",
    "<img src=\"https://lilianweng.github.io/lil-log/assets/images/vae-gaussian.png\">\n",
    "\n",
    "> First, an encoder network turns the input samples x into two parameters in a latent space, which we will note z_mean and z_log_sigma. Then, we randomly sample similar points z from the latent normal distribution that is assumed to generate the data, via z = z_mean + $\\exp$(z_log_sigma) * epsilon, where epsilon is a random normal tensor. Finally, a decoder network maps these latent space points back to the original input data.\n",
    "\n",
    "Here, the loss includes the standard loss (e.g., MSE) for the image reconstruction and a measure of the difference between the distribution of the original data mapped to latent space and the random sampling in latent space. This K-L divergence is a form of information gain, a non-symmetric measure of the difference between two probability distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Lambda\n",
    "\n",
    "latent_dim = 2\n",
    "input_img = Input(shape = (28, 28, 1))\n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
    "\n",
    "\n",
    "# shape info needed to build decoder model\n",
    "shape = K.int_shape(x)\n",
    "\n",
    "# generate latent vector Q(z|X)\n",
    "x = Flatten()(x)\n",
    "x = Dense(16, activation='relu')(x)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "# reparameterization trick\n",
    "# instead of sampling from Q(z|X), sample eps = N(0,I)\n",
    "# then z = z_mean + sqrt(var)*eps\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "# use reparameterization trick to push the sampling out as input\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "# instantiate encoder model\n",
    "encoder = Model(input_img, [z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()\n",
    "\n",
    "# build decoder model\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = Dense(shape[1] * shape[2] * shape[3], activation='relu')(latent_inputs)\n",
    "x = Reshape((shape[1], shape[2], shape[3]))(x)\n",
    "\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "outputs = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "# instantiate decoder model\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()\n",
    "\n",
    "# instantiate VAE model\n",
    "outputs = decoder(encoder(input_img)[2])\n",
    "vae = Model(input_img, outputs, name='vae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = (encoder, decoder)\n",
    "data = (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import mse, binary_crossentropy\n",
    "image_size = x_train.shape[1]\n",
    "\n",
    "# VAE loss = mse + kl_loss\n",
    "reconstruction_loss = mse(K.flatten(input_img), K.flatten(outputs))\n",
    "\n",
    "reconstruction_loss *= image_size * image_size\n",
    "\n",
    "# this is the K-L divergence loss (information gain)\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIT=False\n",
    "\n",
    "if FIT:\n",
    "    vae.fit(x_train,\n",
    "          epochs=50,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(x_test, None))\n",
    "    vae.save_weights('nn_results/colombia_vae_cnn_fashion_70.h5')\n",
    "else:\n",
    "    from keras.models import load_model\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    vae = load_model(\"nn_results/colombia_vae_cnn_fashion_70.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls nn_results/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n",
    "\n",
    "lookup = {0: \"T-shirt/top\",\n",
    "          1: \"Trouser\",\n",
    "          2: \"Pullover\",\n",
    "          3: \"Dress\",\n",
    "          4: \"Coat\",\n",
    "          5: \"Sandal\",\n",
    "          6: \"Shirt\",\n",
    "          7: \"Sneaker\",\n",
    "          8: \"Bag\",\n",
    "          9: \"Ankle boot\"}\n",
    "\n",
    "cmap = plt.cm.get_cmap('coolwarm', 10)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x_test_encoded[0][:,1], x_test_encoded[0][:,0], c=y_test,cmap=cmap, s=4)\n",
    "cbar = plt.colorbar(ticks =np.arange(10), label='clothes type')\n",
    "cbar.ax.set_yticklabels([lookup[i] for i in range(10)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these colored clusters is a type of clothing. Close clusters are clothes that are structurally similar (i.e. clothes that share information in the latent space).\n",
    "\n",
    "Since VAEs are generative models, we can also use it to generate new clothes. Here we will scan the latent plane, sampling latent points at regular intervals, and generating the corresponding piece of clothes for each of these points. This gives us a visualization of the latent manifold that \"generates\" new clothes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a 2D manifold of the clother\n",
    "n = 15  # figure with 15x15 clothes\n",
    "im_size = 28\n",
    "figure = np.zeros((im_size * n,im_size * n))\n",
    "# we will sample n points within [-5, 5] standard deviations\n",
    "grid_x = np.linspace(-5, 5, n)\n",
    "grid_y = np.linspace(-5, 5, n)\n",
    "\n",
    "epsilon_std = 1.0\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]]) * epsilon_std\n",
    "        x_decoded = decoder.predict(z_sample)\n",
    "        cloth = x_decoded[0].reshape(im_size, im_size)\n",
    "        figure[i * im_size: (i + 1) * im_size,\n",
    "               j * im_size: (j + 1) * im_size] = cloth\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "        \n",
    "ax.imshow(figure, cmap=\"viridis\")\n",
    "ax.grid(False)\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
